---
title: "Asto sandbox"
author: "Itay Feldman"
---
```{python}
# Imports & setup
import numpy as np
import pandas as pd
import astropy.units as u
import plotly.express as px
import plotly.graph_objects as go

from astropy.coordinates import SkyCoord
from astropy.io import fits
from astropy.stats import sigma_clipped_stats, mad_std
from dataclasses import dataclass
from photutils.segmentation import detect_sources, deblend_sources, SegmentationImage, SourceCatalog
from photutils.background import Background2D, MedianBackground

from classify import classify_galaxy
from scientific import theta_static, theta_lcdm, get_radius


PIXEL_SCALE = 0.04  # arcsec/pixel

@dataclass
class AstroImage:
    data: np.ndarray
    header: fits.Header
    band: str = ""
    segm: SegmentationImage = None
    catalog: SourceCatalog = None
    bkg: Background2D = None  # Background object from photutils

```

# Data Prep
Start by reading the 4 fits files for the 4 filters
```{python}
# Load the FITS file
files = {
    "b": "./fits/b.fits",
    "i": "./fits/i.fits",
    "u": "./fits/u.fits",
    "v": "./fits/v.fits",
}

def adjust_data(data, threshold_percentile=99):
    """Remove black/low-value edges from image"""
    # Just trust me bro
    data = data[120:, 90:]
    data = data[:, ::-1]  # flip x&y (this is how the image is)
    return data

def read_fits(file_path):
    hdul = fits.open(file_path)
    data = hdul[0].data
    header = hdul[0].header
    hdul.close()
    # FIX ENDIANNESS FIRST before any processing!
    if data.dtype.byteorder == '>':
        data = data.view(data.dtype.newbyteorder()).byteswap()
    # Crop black edges & rotate
    data = adjust_data(data)
    hdul.close()
    return data, header

images: list[AstroImage] = []
for band, file in files.items():
    data, header = read_fits(file)

    images.append(AstroImage(data=data, header=header, band=band))
    print(f"Loaded {band} band - shape: {data.shape}")

# Read planet mask
data, header = read_fits("./data/planet_mask2.fits")
planet_mask = AstroImage(data=data, header=header)

```


Now we want source detection and cataloging

```{python}
# Prepare the mask once (True = mask out, False = keep)
mask = planet_mask.data.astype(bool)

for image in images:
    # Subtract background for better detection
    bkg_estimator = MedianBackground()
    bkg = Background2D(image.data, (50, 50), filter_size=(3, 3), bkg_estimator=bkg_estimator)
    data_sub = image.data - bkg.background
    
    # Store background object for error estimation
    image.bkg = bkg
    
    # Detect sources with proper threshold
    mean, median, std = sigma_clipped_stats(data_sub, sigma=3.0)
    threshold = median + (3 * std)
    
    image.segm = detect_sources(
        data_sub,
        threshold,
        npixels=50,
        mask=mask  # Apply mask DURING detection
    )
    
    # Also remove any segments that overlap with masked regions
    if image.segm is not None:
        # Get unique segment labels that overlap with the mask
        masked_labels = np.unique(image.segm.data[mask])
        masked_labels = masked_labels[masked_labels != 0]  # Exclude background
        
        # Remove those segments
        image.segm.remove_labels(masked_labels)
    
    image.catalog = SourceCatalog(data_sub, image.segm)
    print(f"Band {image.band}: detected {len(image.catalog)} sources (after masking)")
```

Display sources
```{python}
i = 3  # Change this to view different bands (0=b, 1=i, 2=u, 3=v)
sources = images[i].catalog.to_table()
img_data = images[i].data

print(f"Detected {len(sources)} sources in band {images[i].band}")
print(f"Source detection threshold: {threshold:.2f}")

# Use percentile-based contrast stretching (20% to 99%)
vmin, vmax = np.percentile(img_data, [20, 99])

fig = px.imshow(
    img_data, 
    color_continuous_scale='Greys_r',
    zmin=vmin,
    zmax=vmax,
    aspect='equal'
)

# Add detected source markers
fig.add_trace(
    go.Scatter(
        x=sources['xcentroid'],
        y=sources['ycentroid'],
        mode='markers',
        marker=dict(color='red', size=3, symbol='circle-open', line=dict(width=1)),
        name=f'Sources ({len(sources)})'
    )
)

fig.update_layout(
    title=f"Band {images[i].band} - {len(sources)} sources detected",
    width=900,
    height=900,
    coloraxis_colorbar=dict(title="Pixel Value")
)

print(f"Image stats: min={img_data.min():.1f}, max={img_data.max():.1f}, "
      f"median={np.median(img_data):.1f}, std={img_data.std():.1f}")
print(f"Display range: {vmin:.1f} to {vmax:.1f}")
fig.show()
```

Create a combined catalog across all bands
```{python}
# First, create individual band catalogs with positions and fluxes
band_catalogs = {}
for image in images:
    cat = image.catalog.to_table().to_pandas()
    cat['r_half_pix'] = image.catalog.fluxfrac_radius(0.5)
    # segment_flux on background-subtracted data = source-sky (like AstroImageJ)
    cat['source_sky'] = cat['segment_flux']  # Already background-subtracted from data_sub
    
    # Calculate source error like AstroImageJ: sqrt(source_counts + n_pixels * sigma_background^2)
    # Assuming gain ~ 1 for simplicity (typical for modern CCDs in ADU units)
    cat['source_error'] = np.sqrt(np.abs(cat['segment_flux']) + cat['area'] * image.bkg.background_rms_median**2)
    
    # Estimate r_half error as fraction of flux error (r_half scales with sqrt(flux))
    cat['r_half_pix_error'] = cat['r_half_pix'] * cat['source_error'] / (2 * np.abs(cat['segment_flux']) + 1e-10)
    
    band_catalogs[image.band] = cat[['xcentroid', 'ycentroid', 'source_sky', 'source_error', 'r_half_pix', 'r_half_pix_error']].copy()

# Cross-match sources across all bands using position matching
# Start with first band as reference
reference_band = 'b'
matched_sources = []

for idx, ref_source in band_catalogs[reference_band].iterrows():
    ref_x, ref_y = ref_source['xcentroid'], ref_source['ycentroid']
    
    # Try to find matching sources in all other bands (within 3 pixels)
    match_radius = 3.0  # pixels
    matches = {'xcentroid': ref_x, 'ycentroid': ref_y}
    r_half_values = [ref_source['r_half_pix']]
    r_half_pix_errors = [ref_source['r_half_pix_error']]
    
    matches[f'source_sky_{reference_band}'] = ref_source['source_sky']
    matches[f'source_error_{reference_band}'] = ref_source['source_error']
    
    found_in_all_bands = True
    for band in ['i', 'u', 'v']:  # Other bands
        cat = band_catalogs[band]
        # Calculate distance to all sources in this band
        distances = np.sqrt((cat['xcentroid'] - ref_x)**2 + (cat['ycentroid'] - ref_y)**2)
        
        # Find closest match within radius
        if distances.min() < match_radius:
            closest_idx = distances.idxmin()
            matches[f'source_sky_{band}'] = cat.loc[closest_idx, 'source_sky']
            matches[f'source_error_{band}'] = cat.loc[closest_idx, 'source_error']
            r_half_values.append(cat.loc[closest_idx, 'r_half_pix'])
            r_half_pix_errors.append(cat.loc[closest_idx, 'r_half_pix_error'])
        else:
            found_in_all_bands = False
            break
    
    # Only keep sources found in all 4 bands
    if found_in_all_bands:
        # Use median for r_half (robust to outliers)
        r_half_arr = np.array(r_half_values)
        r_half_err_arr = np.array(r_half_pix_errors)
        matches['r_half_pix'] = np.median(r_half_arr)
        
        # Combine measurement errors and scatter for total uncertainty
        measurement_error = np.sqrt(np.sum(r_half_err_arr**2)) / len(r_half_err_arr)  # Propagated errors
        scatter_error = np.std(r_half_arr)  # Scatter between bands
        matches['r_half_pix_error'] = np.sqrt(measurement_error**2 + scatter_error**2)
        matched_sources.append(matches)

# Create clean dataframe with cross-matched sources
cross_matched_catalog = pd.DataFrame(matched_sources)

print(f"Cross-matched catalog has {len(cross_matched_catalog)} sources (present in all 4 bands)")
print(f"Original catalogs had: b={len(band_catalogs['b'])}, i={len(band_catalogs['i'])}, "
      f"u={len(band_catalogs['u'])}, v={len(band_catalogs['v'])} sources")
# display(cross_matched_catalog.head())

```


Now we create another dataframe with the relevant converted SEDs and errors
```{python}
# Conversion factors from source_sky (ADU) to microJy
conversion_factors = {
    'b': 8.8e-18,
    'i': 2.45e-18,
    'u': 5.99e-17,
    'v': 1.89e-18,
}
sed_data = []
for idx, row in cross_matched_catalog.iterrows():
    sed_entry = {'r_half_pix': row['r_half_pix'],
                 'r_half_pix_error': row['r_half_pix_error'],
                 "xcentroid": row['xcentroid'], "ycentroid": row['ycentroid']}
    for band in ['b', 'i', 'u', 'v']:
        flux_adu = row[f'source_sky_{band}']
        error_adu = row[f'source_error_{band}']
        # Convert to microJy
        sed_entry[f'flux_{band}'] = flux_adu * conversion_factors[band]
        sed_entry[f'error_{band}'] = error_adu * conversion_factors[band]
    sed_data.append(sed_entry)
sed_catalog = pd.DataFrame(sed_data)
print(f"SED catalog has {len(sed_catalog)} sources")
# display(sed_catalog.head())

```


Finally, we use this catalog to identify the redshift of each source, and calculate the radius' angle
```{python}
for idx, row in sed_catalog.iterrows():
    # The order of the bands is important here
    fluxes = [row[f'flux_{band}'] for band in ['b', 'i', 'u', 'v']]
    errors = [row[f'error_{band}'] for band in ['b', 'i', 'u', 'v']]

    galaxy_type, redshift = classify_galaxy(fluxes, errors, spectra_path=R".\spectra")

    sed_catalog.at[idx, 'redshift'] = redshift
    sed_catalog.at[idx, 'galaxy_type'] = galaxy_type

    # Calculate angular size in arcseconds
    r_half_pix, r_half_pix_error = row['r_half_pix'], row['r_half_pix_error']
    r_half_arcsec = r_half_pix * PIXEL_SCALE
    r_half_arcsec_error = r_half_pix_error * PIXEL_SCALE
    sed_catalog.at[idx, 'r_half_arcsec'] = r_half_arcsec
    sed_catalog.at[idx, 'r_half_arcsec_error'] = r_half_arcsec_error

display(sed_catalog.head())
```

# Finally!
We can finally use the calculated angle and plot the static and lambda-CDM models to show overall behavior

```{python}
# fig = px.scatter(
#     sed_catalog,
#     x='redshift',
#     y='r_half_arcsec',
#     error_y='r_half_arcsec_error',
#     title='Galaxy Half-Light Radius vs Redshift',
#     labels={'redshift': 'Redshift (z)', 'r_half_arcsec': 'Half-Light Radius (arcsec)'},
#     log_y=True
# )
# fig.show()
```

```{python}
sed_catalog = sed_catalog.dropna(subset=["redshift", "r_half_arcsec", "r_half_arcsec_error"])

# Guard against degenerate redshift range
z_min_raw, z_max_raw = sed_catalog.redshift.min(), sed_catalog.redshift.max()
if not np.isfinite(z_min_raw) or not np.isfinite(z_max_raw) or z_min_raw == z_max_raw:
    raise ValueError("Cannot bin redshifts: invalid or zero range")

bins = np.linspace(z_min_raw, z_max_raw, 6)
sed_catalog["z_bin"] = pd.cut(sed_catalog.redshift, bins, include_lowest=True)

binned = (
    sed_catalog.groupby("z_bin", observed=False)
    .agg(
        z_mid=("redshift", "median"),
        theta_med=("r_half_arcsec", "median"),
        theta_err=("r_half_arcsec_error", lambda x: np.sqrt(np.mean(np.square(x)))),
        theta_lo=("r_half_arcsec", lambda x: np.percentile(x, 25) if len(x) else np.nan),
        theta_hi=("r_half_arcsec", lambda x: np.percentile(x, 75) if len(x) else np.nan),
        n=("redshift", "count"),
    )
    .reset_index(drop=True)
)

# Drop bins with insufficient data and enforce positive z for models
binned = binned.dropna(subset=["z_mid", "theta_med", "theta_err"])
binned = binned[(binned.n > 0) & (binned.z_mid > 0)]

if len(binned) < 2:
    raise ValueError("Not enough binned data to fit models")

z_min = max(1e-4, binned.z_mid.min())
z_model = np.linspace(z_min, binned.z_mid.max(), 300)

# Fit characteristic size for each cosmology using binned medians
R_static = get_radius(binned.z_mid.values, binned.theta_med.values,
                      binned.theta_err.values, model='static')
R_lcdm = get_radius(binned.z_mid.values, binned.theta_med.values,
                    binned.theta_err.values, model='lcdm')

# Model curves (convert radians -> arcsec)
theta_static_model = theta_static(z_model, R_static)
theta_lcdm_model = theta_lcdm(z_model, R_lcdm)

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=binned["z_mid"],
    y=binned["theta_med"],
    mode="markers+lines",
    name="Median angular size",
    error_y=dict(
        type="data",
        symmetric=True,
        array=binned["theta_err"],
        arrayminus=binned["theta_err"]
    )
))

# Add model predictions
fig.add_trace(go.Scatter(
    x=z_model,
    y=theta_static_model,
    mode="lines",
    # name=f"Static model (R={R_static:.3f} Mpc)",
    name=f"Static model",
    line=dict(color="gray", dash="dash")
))

fig.add_trace(go.Scatter(
    x=z_model,
    y=theta_lcdm_model,
    mode="lines",
    # name=f"ΛCDM model (R={R_lcdm:.3f} Mpc)",
    name=f"ΛCDM model",
    line=dict(color="blue")
))

fig.update_layout(
    title="Median galaxy angular size vs redshift",
    xaxis_title="Redshift z",
    yaxis_title="Angular size θ (arcsec)"
)

fig.show()
```